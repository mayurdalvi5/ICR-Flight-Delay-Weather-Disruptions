{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Dataset/Illinois_10_years_data.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Downsample the data to reduce size (e.g., sample 1% of the data)\n",
        "df = df.sample(frac=0.01, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Convert integer features to int32 for memory efficiency\n",
        "int_columns = ['Year', 'Quarter', 'Month', 'Day_of_Month', 'Day_of_Week', 'Scheduled_Departure_Time', 'Scheduled_Departure_Time_Minutes', 'Target']\n",
        "for col in int_columns:\n",
        "    df[col] = df[col].astype(np.int32)\n",
        "\n",
        "# Convert continuous numeric features to float32\n",
        "float_columns = ['Taxi_Out_Time_Minutes', 'Flight_Distance_Miles', 'Air_Temperature_Fahrenheit', 'Dew_Point_Temperature_Fahrenheit',\n",
        "                 'Relative_Humidity_Percent', 'Wind_Direction_Degrees', 'Wind_Speed_Knots', 'Hourly_Precipitation_Inches',\n",
        "                 'Pressure_Altimeter_Inches', 'Sea_Level_Pressure_Millibar', 'Visibility_Miles', 'Sky_Level_1_Altitude_Feet',\n",
        "                 'Apparent_Temperature_Fahrenheit']\n",
        "for col in float_columns:\n",
        "    df[col] = df[col].astype(np.float32)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(['Origin_State_Name', 'Departure_Datetime', 'Departure_Delay_Minutes'], axis=1)\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_columns = ['Operating_Carrier_Code', 'Tail_Number', 'Origin_Airport_ID', 'Origin_Airport_Code', 'Destination_Airport_Code', 'Destination_State_Name', 'Sky_Cover_Level_1']\n",
        "\n",
        "# Separate high-cardinality and low-cardinality categorical columns\n",
        "high_cardinality_columns = ['Tail_Number', 'Origin_Airport_Code', 'Destination_Airport_Code']\n",
        "low_cardinality_columns = [col for col in categorical_columns if col not in high_cardinality_columns]\n",
        "\n",
        "# Apply label encoding to high-cardinality categorical features\n",
        "label_encoders = {}\n",
        "for col in high_cardinality_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Apply one-hot encoding to low-cardinality categorical features\n",
        "df = pd.get_dummies(df, columns=low_cardinality_columns, drop_first=True)\n",
        "\n",
        "# Define the years for each dataset\n",
        "train_years = [2014, 2015, 2016, 2017, 2018, 2019]\n",
        "val_years = [2020, 2021, 2022]\n",
        "test_years = [2023, 2024]\n",
        "\n",
        "# Create train, validation, and test sets\n",
        "train_df = df[df['Year'].isin(train_years)].reset_index(drop=True)\n",
        "val_df = df[df['Year'].isin(val_years)].reset_index(drop=True)\n",
        "test_df = df[df['Year'].isin(test_years)].reset_index(drop=True)\n",
        "\n",
        "# Separate features and target variable\n",
        "X_train = train_df.drop('Target', axis=1)\n",
        "y_train = train_df['Target']\n",
        "X_val = val_df.drop('Target', axis=1)\n",
        "y_val = val_df['Target']\n",
        "X_test = test_df.drop('Target', axis=1)\n",
        "y_test = test_df['Target']\n",
        "\n",
        "# Standardize numeric features to improve SVM model performance\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train a simple SVM model\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate on validation set\n",
        "y_val_pred = svm_model.predict(X_val_scaled)\n",
        "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"\\nClassification Report on Validation Set:\\n\", classification_report(y_val, y_val_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
        "\n",
        "# Evaluate on test set\n",
        "y_test_pred = svm_model.predict(X_test_scaled)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
        "print(\"\\nClassification Report on Test Set:\\n\", classification_report(y_test, y_test_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 543
        },
        "id": "m8pxVPEK9K5K",
        "outputId": "8acdf29c-fc6d-4688-8f77-8f0a69eaa576"
      },
      "id": "m8pxVPEK9K5K",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-8078b76e2764>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/Dataset/Illinois_10_years_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Drop rows with missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the Validation Set:**\n",
        "\n",
        "* We get an accuracy of 86.3% which seems high, but it's misleading. This accuracy reflects that the model is heavily biased towards predicting the majority class (no delay), which dominates the dataset.\n",
        "\n",
        "* Class 0 (No Delay): The model has high precision (0.86) and recall (1.00), meaning it correctly identifies non-delayed flights.\n",
        "\n",
        "* Class 1 (Delay): Precision and recall are both 0, indicating that the model fails to identify any delays. This likely occurs because of class imbalanceâ€”there are far fewer delayed flights compared to non-delayed ones, so the model defaults to predicting \"No Delay\" for all instances.\n",
        "\n",
        "* The confusion matrix shows that all 946 delayed flights were classified as non-delayed, with no true positives for the delay class.\n",
        "\n",
        "**For the test set:**\n",
        "\n",
        "* : We get an accuracy of 77%, which again seems reasonable at first glance but is similarly biased.\n"
      ],
      "metadata": {
        "id": "53RQy4zRVjb7"
      },
      "id": "53RQy4zRVjb7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, we need to Oversample the minority class (delays) or undersample the majority class (no delay). We can use the technique SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class."
      ],
      "metadata": {
        "id": "KZMSH1kAWByq"
      },
      "id": "KZMSH1kAWByq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/Dataset/Illinois_10_years_data.csv')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Downsample to 10% for performance (adjust as needed)\n",
        "df = df.sample(frac=0.1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Convert integer features to int32 for memory efficiency\n",
        "int_columns = ['Year', 'Quarter', 'Month', 'Day_of_Month', 'Day_of_Week', 'Scheduled_Departure_Time', 'Scheduled_Departure_Time_Minutes', 'Target']\n",
        "for col in int_columns:\n",
        "    df[col] = df[col].astype(np.int32)\n",
        "\n",
        "# Convert continuous numeric features to float32\n",
        "float_columns = ['Taxi_Out_Time_Minutes', 'Flight_Distance_Miles', 'Air_Temperature_Fahrenheit', 'Dew_Point_Temperature_Fahrenheit',\n",
        "                 'Relative_Humidity_Percent', 'Wind_Direction_Degrees', 'Wind_Speed_Knots', 'Hourly_Precipitation_Inches',\n",
        "                 'Pressure_Altimeter_Inches', 'Sea_Level_Pressure_Millibar', 'Visibility_Miles', 'Sky_Level_1_Altitude_Feet',\n",
        "                 'Apparent_Temperature_Fahrenheit']\n",
        "for col in float_columns:\n",
        "    df[col] = df[col].astype(np.float32)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(['Origin_State_Name', 'Departure_Datetime', 'Departure_Delay_Minutes'], axis=1)\n",
        "\n",
        "# Define categorical columns\n",
        "categorical_columns = ['Operating_Carrier_Code', 'Tail_Number', 'Origin_Airport_ID', 'Origin_Airport_Code', 'Destination_Airport_Code', 'Destination_State_Name', 'Sky_Cover_Level_1']\n",
        "\n",
        "# Apply label encoding to high-cardinality categorical features\n",
        "label_encoders = {}\n",
        "for col in categorical_columns:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Target', axis=1)\n",
        "y = df['Target']\n",
        "\n",
        "# Apply SMOTE to balance the classes\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model = SVC(kernel='linear', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = svm_model.predict(X_test_scaled)\n",
        "y_pred_prob = svm_model.predict_proba(X_test_scaled)[:,1]\n",
        "\n",
        "# Classification Report and Confusion Matrix\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
        "\n",
        "# Visualization Metrics\n",
        "# 1. Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['No Delay', 'Delay'], yticklabels=['No Delay', 'Delay'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# 2. ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(fpr, tpr, color='blue', label='AUC = %0.2f' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# 3. Precision-Recall Curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(recall, precision, color='purple')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xuLJ4ppWOhan"
      },
      "id": "xuLJ4ppWOhan",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}