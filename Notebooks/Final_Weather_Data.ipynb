{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QE5sSPr0b7Un"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import csv\n",
        "import codecs\n",
        "import pandas as pd\n",
        "import calendar\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weather Data Collection Function\n",
        "def fetch_weather_data(station_ids, start_date, end_date):\n",
        "    \"\"\"Download weather data for specified stations and date range, filtering out rows with missing temperature (tmpf).\"\"\"\n",
        "    localfn = f\"weather_data_{start_date.strftime('%Y%m%d')}_{end_date.strftime('%Y%m%d')}_filtered.csv\"\n",
        "\n",
        "    if os.path.isfile(localfn):\n",
        "        print(f\"- File already exists: {localfn}\")\n",
        "        return localfn\n",
        "\n",
        "    print(f\"+ Downloading data for stations {', '.join(station_ids)} from {start_date} to {end_date}\")\n",
        "\n",
        "    stations_str = \",\".join(station_ids)\n",
        "    uri = (\n",
        "        \"https://mesonet.agron.iastate.edu/cgi-bin/request/asos.py?\"\n",
        "        f\"station={stations_str}&\"\n",
        "        \"data=tmpf,dwpf,relh,drct,sknt,p01i,alti,mslp,vsby,gust,\"\n",
        "        \"skyc1,skyc2,skyc3,skyc4,skyl1,skyl2,skyl3,skyl4,wxcodes,\"\n",
        "        \"ice_accretion_1hr,ice_accretion_3hr,ice_accretion_6hr,\"\n",
        "        \"peak_wind_gust,peak_wind_drct,peak_wind_time,feel,metar,snowdepth&\"\n",
        "        f\"year1={start_date.year}&month1={start_date.month}&day1={start_date.day}&\"\n",
        "        f\"year2={end_date.year}&month2={end_date.month}&day2={end_date.day}&\"\n",
        "        \"tz=UTC&format=onlycomma&latlon=no&elev=no&missing=M&trace=T&direct=yes\"\n",
        "    )\n",
        "\n",
        "    with requests.get(uri, stream=True, timeout=300) as res:\n",
        "        if res.status_code == 200:\n",
        "            decoded_stream = codecs.iterdecode(res.iter_lines(), 'utf-8')\n",
        "            reader = csv.reader(decoded_stream)\n",
        "            headers = next(reader)\n",
        "\n",
        "            tmpf_index = headers.index('tmpf') if 'tmpf' in headers else None\n",
        "\n",
        "            if tmpf_index is None:\n",
        "                print(\"Error: 'tmpf' column not found.\")\n",
        "                return\n",
        "\n",
        "            with open(localfn, \"w\", newline='', encoding=\"utf-8\") as fh:\n",
        "                writer = csv.writer(fh)\n",
        "                writer.writerow(headers)\n",
        "\n",
        "                row_count = 0\n",
        "                for row in reader:\n",
        "                    if len(row) == len(headers):\n",
        "                        tmpf_value = row[tmpf_index]\n",
        "                        if tmpf_value != 'M':  # Filter out rows with missing temperature\n",
        "                            writer.writerow(row)\n",
        "                            row_count += 1\n",
        "\n",
        "            print(f\"+ Data saved to {localfn} with {row_count} rows (excluding missing temperatures).\")\n",
        "        else:\n",
        "            print(f\"Failed to download data: {res.status_code}\")\n",
        "\n",
        "    return localfn\n"
      ],
      "metadata": {
        "id": "BLvDJrtdcHQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TVozetDCtg48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Weather Data Cleaning Function\n",
        "def clean_weather_data(file_path):\n",
        "    \"\"\"Clean the collected weather data by handling missing values.\"\"\"\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Replace 'M' with NaN\n",
        "    df.replace('M', pd.NA, inplace=True)\n",
        "\n",
        "    df['p01i'] = df['p01i'].replace('T', 0.005)\n",
        "    df['skyl1'] = df['skyl1'].replace('NaN', 0)  # Replace 'M' (missing) with 0 feet for clear skies\n",
        "    df['valid'] = pd.to_datetime(df['valid'])\n",
        "\n",
        "\n",
        "    # Convert weather columns to appropriate types\n",
        "    weather_features = ['tmpf', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'alti', 'mslp', 'vsby', 'gust', 'skyl1']\n",
        "    for feature in weather_features:\n",
        "        df[feature] = pd.to_numeric(df[feature], errors='coerce')\n",
        "\n",
        "    df.sort_values(by=['station', 'valid'], inplace = True)\n",
        "\n",
        "    # List of columns to apply spline interpolation\n",
        "    columns_to_interpolate = ['mslp', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'vsby', 'skyl1', 'feel']\n",
        "\n",
        "    # Apply spline interpolation for each station group separately\n",
        "    for col in columns_to_interpolate:\n",
        "        df[col] = df.groupby('station')[col].transform(lambda group: group.interpolate(method='linear'))\n",
        "\n",
        "    # Option 1: Forward Fill the remaining missing values\n",
        "    df[['mslp','skyl1','feel','skyc1']] = df[['mslp','skyl1','feel','skyc1']].ffill()\n",
        "\n",
        "    # Option 2: Backward Fill the remaining missing values (optional, can be combined with ffill)\n",
        "    df[['mslp','skyl1','feel','skyc1']] = df[['mslp','skyl1','feel','skyc1']].bfill()\n",
        "\n",
        "    columns_to_drop = ['snowdepth','wxcodes','skyl2','skyl3','skyl4','skyc2','skyc3','skyc4',\n",
        "                    'gust','ice_accretion_1hr','ice_accretion_3hr','ice_accretion_6hr',\n",
        "                    'peak_wind_gust','peak_wind_drct','peak_wind_time','metar']\n",
        "    # Drop the columns\n",
        "    df.drop(columns=columns_to_drop, inplace=True)\n",
        "\n",
        "    # # Handle missing values\n",
        "    # print(\"Missing values after handling:\")\n",
        "    # print(df.isnull().sum())\n",
        "\n",
        "    # Save cleaned data\n",
        "    cleaned_file = file_path.replace(\"filtered\", \"cleaned\")\n",
        "    df.to_csv(cleaned_file, index=False)\n",
        "    print(f\"Cleaned data saved to {cleaned_file}\")\n",
        "\n",
        "    return cleaned_file"
      ],
      "metadata": {
        "id": "aU8Ic6jpcMny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resample_to_15min(df):\n",
        "    \"\"\"Resample the dataframe to 15-minute intervals for each station and fill missing data.\"\"\"\n",
        "\n",
        "    # Ensure 'valid' is datetime\n",
        "    df['resample15'] = pd.to_datetime(df['valid'])\n",
        "\n",
        "    # Set 'valid' as the index (so we can easily reindex later)\n",
        "    df.set_index('resample15', inplace=True)\n",
        "\n",
        "    # Create a DataFrame to store the results\n",
        "    full_df = pd.DataFrame()\n",
        "\n",
        "    # List of numeric and categorical columns\n",
        "    numeric_cols = ['tmpf','alti','mslp', 'dwpf', 'relh', 'drct', 'sknt', 'p01i', 'vsby', 'skyl1', 'feel']\n",
        "    categorical_cols = ['skyc1','valid']\n",
        "\n",
        "    # Apply resampling for each station separately\n",
        "    for station, station_data in df.groupby('station'):\n",
        "        # Generate a full range of 15-minute intervals for this station\n",
        "        start_time = station_data.index.min()\n",
        "        end_time = station_data.index.max()\n",
        "        full_time_range = pd.date_range(start=start_time, end=end_time, freq='15min')\n",
        "\n",
        "        # Reindex station_data with the full 15-minute interval range\n",
        "        station_data = station_data.reindex(full_time_range)\n",
        "\n",
        "        # Fill missing 'station' column with the current station name\n",
        "        station_data['station'] = station\n",
        "\n",
        "        # Interpolate missing numeric values for each column\n",
        "        station_data[numeric_cols] = station_data[numeric_cols].interpolate(method='linear')\n",
        "\n",
        "        # Use forward fill and backward fill to ensure no missing values remain\n",
        "        station_data[numeric_cols] = station_data[numeric_cols].ffill().bfill()\n",
        "\n",
        "        # Forward fill and backward fill categorical columns\n",
        "        station_data[categorical_cols] = station_data[categorical_cols].ffill().bfill()\n",
        "\n",
        "        # Append to the final DataFrame\n",
        "        full_df = pd.concat([full_df, station_data])\n",
        "\n",
        "        # Round the 'mslp' column to the nearest one decimal place\n",
        "        full_df['mslp'] = full_df['mslp'].round(1)\n",
        "        full_df['feel'] = full_df['feel'].round(2)\n",
        "        full_df['relh'] = full_df['relh'].round(2)\n",
        "        full_df['alti'] = full_df['alti'].round()\n",
        "        full_df['sknt'] = full_df['sknt'].round()\n",
        "\n",
        "        full_df['drct'] = (full_df['drct'] / 10).round() * 10\n",
        "        # Custom rounding function to round 'skyl1' to the nearest 100\n",
        "        full_df['skyl1'] = full_df['skyl1'].apply(lambda x: round(x / 100) * 100)\n",
        "\n",
        "\n",
        "\n",
        "    # Reset index to bring 'valid' back as a column\n",
        "    full_df.reset_index(inplace=True)\n",
        "    full_df.rename(columns={'index': 'resample15'}, inplace=True)\n",
        "\n",
        "    full_df['resample15'] = pd.to_datetime(full_df['resample15'], format='%Y-%m-%d %H:%M')\n",
        "    # Round the 'resample15' column to the nearest 15-minute interval\n",
        "    full_df['resample15'] = full_df['resample15'].dt.round('15min')\n",
        "\n",
        "    #Dropping duplicates row after rounding up time\n",
        "\n",
        "    full_df[['station', 'resample15']] = full_df[['station', 'resample15']].drop_duplicates()\n",
        "\n",
        "\n",
        "    return full_df\n"
      ],
      "metadata": {
        "id": "6s1HLnkr8_c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    station_ids = ['DEN', 'EGE', 'GUC', 'MTJ', 'COS', 'HDN', 'GJT', 'DRO', 'ASE', 'JFK', 'LGA', 'SYR', 'BUF', 'ALB', 'ROC', 'ITH', 'HPN', 'BGM',\n",
        "                   'ISP', 'IAG', 'PBG', 'ELM', 'SWF',\n",
        "                   'ACV','BFL','BIH','BUR','FAT','LAX','LGB','MRY','OAK','ONT','PSP','RDD','SAN','SBA','SBP','SCK','SFO','SJC','SMF','SMX','SNA','STS',\n",
        "                   'DAB','ECP','EYW','FLL','GNV','JAX','MCO','MIA','MLB','PBI','PGD','PIE','PNS','RSW','SFB','SRQ','TLH','TPA','VPS',\n",
        "                   'ABI','ACT','AMA','AUS','BPT','BRO','CLL','CRP','DAL','DFW','ELP','GGG','GRK','HOU','HRL','IAH','LBB','LRD','MAF','MFE','SAT','SJT','SPS','TYR','VCT',\n",
        "                   'BLV','BMI','CMI','DEC','MDW','MLI','ORD','PIA','RFD','SPI',\n",
        "                   'ABY','AGS','ATL','BQK','CSG','SAV','VLD',\n",
        "                   'ACY','EWR','TTN',\n",
        "                   'BWI','HGR',\n",
        "                   'EKO','LAS','RNO']  # Sample station IDs [ New york, Colorado, California, Florida, Texas, Illionis, Georgia, New Jersey, Maryland, Nevada   ]\n",
        "    year = 2024\n",
        "    month = 1\n",
        "    start_date = datetime(year, month, 1)\n",
        "    first_weekday, num_days = calendar.monthrange(year, month)\n",
        "    end_date = start_date + timedelta(days=num_days)\n",
        "\n",
        "    # Step 1: Collect Weather Data\n",
        "    file_path = fetch_weather_data(station_ids, start_date, end_date)\n",
        "\n",
        "    # Step 2: Clean the Collected Weather Data\n",
        "    df = clean_weather_data(file_path)\n",
        "    df = pd.read_csv(df)\n",
        "    # Step 3: Resample the 'valid' column to 15-minute intervals and interpolate\n",
        "    df = resample_to_15min(df)\n",
        "\n",
        "\n",
        "    # Step 4: Save the final DataFrame to a CSV file\n",
        "    cleaned_file = file_path.replace(\"filtered\", \"cleaned_15min\")\n",
        "    df.to_csv(f'{cleaned_file}', index=False)\n",
        "    print(f\"Cleaned and resampled data saved to {cleaned_file}\")"
      ],
      "metadata": {
        "id": "skDuR3_5cQFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLgm_LPgcTP8",
        "outputId": "81dd05f5-1838-4242-a2f6-6a7db25cd715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- File already exists: weather_data_20240101_20240201_filtered.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-f877b4ef879c>:4: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path)\n",
            "<ipython-input-23-f877b4ef879c>:26: FutureWarning: Series.interpolate with object dtype is deprecated and will raise in a future version. Call obj.infer_objects(copy=False) before interpolating instead.\n",
            "  df[col] = df.groupby('station')[col].transform(lambda group: group.interpolate(method='linear'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned data saved to weather_data_20240101_20240201_cleaned.csv\n",
            "Cleaned and resampled data saved to weather_data_20240101_20240201_cleaned_15min.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boeBAgFWq3oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2m79GNaVF2Fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}